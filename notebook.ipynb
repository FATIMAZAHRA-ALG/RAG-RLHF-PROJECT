{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a75047",
   "metadata": {},
   "source": [
    "# üìò Syst√®mes de QA : RAG et RLHF\n",
    "\n",
    "Dans ce notebook, nous pr√©sentons **deux techniques distinctes** pour la construction de syst√®mes de Question-R√©ponse (QA), chacune √©tant **ind√©pendante** de l'autre. Elles illustrent deux approches diff√©rentes pour produire des r√©ponses pertinentes √† partir d‚Äôune question pos√©e.\n",
    "\n",
    "\n",
    "\n",
    "üîπ **Premi√®re partie ‚Äì RAG (Retrieval-Augmented Generation)**  \n",
    "Cette premi√®re partie introduit l‚Äôapproche RAG, qui combine la r√©cup√©ration d‚Äôinformations pertinentes √† partir de documents avec la g√©n√©ration de texte.  \n",
    "Le principe consiste √† enrichir la r√©ponse √† une question en s‚Äôappuyant sur des passages extraits dynamiquement d‚Äôune source externe.\n",
    "\n",
    "\n",
    "\n",
    "üîπ **Deuxi√®me partie ‚Äì RLHF (Reinforcement Learning with Human Feedback)**  \n",
    "La deuxi√®me partie explore une autre approche, bas√©e sur l'apprentissage par renforcement guid√© par des retours humains.  \n",
    "Elle vise √† affiner un mod√®le de g√©n√©ration pour qu‚Äôil produise des r√©ponses plus align√©es avec des attentes humaines, en s‚Äôappuyant sur des techniques d‚Äôoptimisation via feedback.\n",
    "\n",
    "\n",
    "\n",
    "‚úÖ Ces deux approches sont trait√©es **s√©par√©ment** dans ce notebook. Chacune a son propre pipeline, ses mod√®les et ses donn√©es.\n",
    "\n",
    "‚û°Ô∏è Commen√ßons maintenant par la premi√®re approche : **RAG (Retrieval-Augmented Generation)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90c11b",
   "metadata": {},
   "source": [
    "# I-La technique RAG\n",
    "## üîß √âtapes principales du pipeline\n",
    "\n",
    "1. Importation des biblioth√®ques n√©cessaires  \n",
    "2. Extraction du texte depuis un fichier PDF  \n",
    "3. Division du texte en paragraphes  \n",
    "4. Cr√©ation des repr√©sentations vectorielles (embeddings) \n",
    "5. Indexation vectorielle \n",
    "6. Recherche des passages pertinents  \n",
    "7. Construction du contexte  \n",
    "8. G√©n√©ration de la r√©ponse avec un mod√®le flan-T5-base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51957d9",
   "metadata": {},
   "source": [
    "##  1. Importations des biblioth√®ques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4573c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  \n",
    "import re\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee100f",
   "metadata": {},
   "source": [
    "##  2. Fonctions de traitement de texte\n",
    "### üîπ Extraction du texte depuis le PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b982fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    doc.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcaceaa",
   "metadata": {},
   "source": [
    "### üîπ D√©coupage du texte en paragraphes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_paragraphs(full_text, max_chars=200):\n",
    "    paragraphs = []\n",
    "    start = 0\n",
    "    while start < len(full_text):\n",
    "        end = start + max_chars\n",
    "        if end < len(full_text):\n",
    "            while end > start and full_text[end] not in [' ', '.', '\\n']:\n",
    "                end -= 1\n",
    "            if end == start:\n",
    "                end = start + max_chars\n",
    "        paragraph = full_text[start:end].strip()\n",
    "        paragraphs.append(paragraph)\n",
    "        start = end\n",
    "    return paragraphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d3e81",
   "metadata": {},
   "source": [
    "##  3. Fonctions pour l'encodage et l'indexation\n",
    "### üîπ Cr√©ation des vecteurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e91080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(paragraphs, embedding_model):\n",
    "    return embedding_model.encode(paragraphs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8bbda0",
   "metadata": {},
   "source": [
    "### üîπ Cr√©ation de l‚Äôindex FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04adb997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings):\n",
    "    embeddings_np = np.array(embeddings).astype('float32')\n",
    "    index = faiss.IndexFlatL2(embeddings_np.shape[1])\n",
    "    index.add(embeddings_np)\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a351945",
   "metadata": {},
   "source": [
    "### üîπ Recherche dans l‚Äôindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec4b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss_index(query, index, embedding_model, k):\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    scores, indices = index.search(np.array([query_embedding]).astype('float32'), k)\n",
    "    return indices[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a6ddc",
   "metadata": {},
   "source": [
    "## 4. G√©n√©ration de la r√©ponse avec un mod√®le flan-T5-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e069ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, context, qa_pipeline):\n",
    "    prompt = (\n",
    "        f\"R√©ponds √† la question suivante en utilisant le contexte fourni.\\n\"\n",
    "        f\"Contexte : {context}\\n\"\n",
    "        f\"Question : {question}\"\n",
    "    )\n",
    "    return qa_pipeline(prompt, max_new_tokens=150, truncation=True)[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeedd5a2",
   "metadata": {},
   "source": [
    "# 5. Pipeline Principal\n",
    "Voici l‚Äôex√©cution compl√®te du syst√®me RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Pipeline Principal ===\n",
    "if __name__ == \"__main__\":\n",
    "        # 1. Sp√©cifier le chemin vers le fichier PDF\n",
    "    pdf_path = \"C:/Users/fatim/OneDrive/Bureau/RAG_Project/d√©finitions_g√©n√©rales.pdf\"\n",
    "\n",
    "    # 2. Extraction du texte et d√©coupage\n",
    "    full_text = extract_text_from_pdf(pdf_path)\n",
    "    paragraphs = split_text_into_paragraphs(full_text)\n",
    "\n",
    "    # Affichage du nombre de paragraphes extraits\n",
    "    print(f\"Nombre de paragraphes extraits : {len(paragraphs)}\\n\")\n",
    "\n",
    "    # Affichage des premiers paragraphes extraits pour v√©rification\n",
    "    print(\"Quelques paragraphes extraits :\\n\")\n",
    "    for i, paragraph in enumerate(paragraphs[:5]):  # Afficher les 5 premiers paragraphes\n",
    "        print(f\"Paragraphe {i+1}: {paragraph}\\n\")\n",
    "\n",
    "    # 3. Embedding\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = create_embeddings(paragraphs, embedding_model)\n",
    "\n",
    "    # 4. Indexation\n",
    "    index = create_faiss_index(embeddings)\n",
    "\n",
    "    # 5. Recherche d'information\n",
    "    query = \"C'est quoi l'innovation ?\"\n",
    "    k = 3\n",
    "    relevant_indices = search_faiss_index(query, index, embedding_model, k)\n",
    "    relevant_indices = [int(i) for i in relevant_indices if i >= 0]\n",
    "\n",
    "    print(\"La question pos√©e : \",query)\n",
    "\n",
    "    # Affichage des indices des passages pertinents\n",
    "    print(f\"Indices des passages pertinents : {relevant_indices}\\n\")\n",
    "\n",
    "    # Affichage des passages pertinents\n",
    "    print(\"Passages pertinents extraits :\\n\")\n",
    "    for i in relevant_indices:\n",
    "        print(f\"Passage {i+1}: {paragraphs[i]}\\n\")\n",
    "\n",
    "    # 6. Construction du contexte\n",
    "    context = \"\\n\".join([paragraphs[i] for i in relevant_indices])\n",
    "\n",
    "    # Affichage du contexte\n",
    "    print(f\"Contexte utilis√© pour g√©n√©rer la r√©ponse : \\n{context}\\n\")\n",
    "\n",
    "    # 7. G√©n√©ration de la r√©ponse\n",
    "    qa_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "    answer = generate_answer(query, context, qa_pipeline)\n",
    "\n",
    "    # Affichage de la r√©ponse g√©n√©r√©e\n",
    "    print(\"\\nR√©ponse g√©n√©r√©e :\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292a9577",
   "metadata": {},
   "source": [
    "# Resultat de l'ex√©cution du pipeline principal :\n",
    "\n",
    "Nombre de paragraphes extraits : 14\n",
    "\n",
    "Quelques paragraphes extraits :\n",
    "\n",
    "Paragraphe 1: L‚Äôintelligence artificielle (IA) est un ensemble de techniques permettant √† des machines d‚Äôimiter\n",
    "des fonctions cognitives humaines telles que l‚Äôapprentissage, le raisonnement, la r√©solution de\n",
    "\n",
    "Paragraphe 2: probl√®mes ou la compr√©hension du langage.\n",
    "L‚ÄôADN (acide d√©soxyribonucl√©ique) est une mol√©cule pr√©sente dans toutes les cellules vivantes et qui\n",
    "contient les instructions g√©n√©tiques n√©cessaires au\n",
    "\n",
    "Paragraphe 3: d√©veloppement et au fonctionnement des\n",
    "organismes.\n",
    "Le droit p√©nal est la branche du droit qui d√©finit les infractions et d√©termine les peines\n",
    "applicables aux personnes qui les commettent.\n",
    "Le\n",
    "\n",
    "Paragraphe 4: r√©chauffement climatique d√©signe l‚Äôaugmentation progressive des temp√©ratures moyennes √† la\n",
    "surface de la Terre, principalement caus√©e par les activit√©s humaines et les √©missions de gaz √†\n",
    "effet de\n",
    "\n",
    "Paragraphe 5: serre.\n",
    "La d√©mocratie est un syst√®me politique dans lequel le pouvoir est exerc√© par le peuple, soit\n",
    "directement, soit par l‚Äôinterm√©diaire de repr√©sentants √©lus.\n",
    "Un algorithme est une suite finie\n",
    "\n",
    "La question pos√©e :  C'est quoi l'innovation ?\n",
    "\n",
    "Indices des passages pertinents : [11, 12, 10]\n",
    "\n",
    "Passages pertinents extraits :\n",
    "\n",
    "Passage 12: concepts.\n",
    "L‚Äôinnovation est l‚Äôintroduction d‚Äôune nouveaut√© (produit, service, proc√©d√© ou organisation) qui\n",
    "apporte une am√©lioration significative par rapport √† l‚Äôexistant.\n",
    "Une r√©cession est une baisse\n",
    "\n",
    "Passage 13: prolong√©e de l‚Äôactivit√© √©conomique, g√©n√©ralement mesur√©e par une\n",
    "diminution du PIB pendant au moins deux trimestres cons√©cutifs.\n",
    "Une machine virtuelle est un environnement logiciel qui simule un\n",
    "\n",
    "Passage 11: cyberattaques.\n",
    "L‚Äôabstraction en art d√©signe un style dans lequel les formes, les couleurs et les lignes ne\n",
    "repr√©sentent pas directement la r√©alit√©, mais expriment des id√©es, des √©motions ou des\n",
    "\n",
    "Contexte utilis√© pour g√©n√©rer la r√©ponse :\n",
    "concepts.\n",
    "L‚Äôinnovation est l‚Äôintroduction d‚Äôune nouveaut√© (produit, service, proc√©d√© ou organisation) qui\n",
    "apporte une am√©lioration significative par rapport √† l‚Äôexistant.\n",
    "Une r√©cession est une baisse\n",
    "prolong√©e de l‚Äôactivit√© √©conomique, g√©n√©ralement mesur√©e par une\n",
    "diminution du PIB pendant au moins deux trimestres cons√©cutifs.\n",
    "Une machine virtuelle est un environnement logiciel qui simule un\n",
    "cyberattaques.\n",
    "L‚Äôabstraction en art d√©signe un style dans lequel les formes, les couleurs et les lignes ne\n",
    "repr√©sentent pas directement la r√©alit√©, mais expriment des id√©es, des √©motions ou des\n",
    "\n",
    "Device set to use cpu\n",
    "\n",
    "R√©ponse g√©n√©r√©e :\n",
    " l‚Äôintroduction d‚Äôune nouveaut√© (produit, service, proc√©d√© ou organisation) qui apporte une am√©lioration significative par rapport √† l‚Äôexistant "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e17a",
   "metadata": {},
   "source": [
    "# II- La technique RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a28288",
   "metadata": {},
   "source": [
    "## üîß √âtapes principales du pipeline\n",
    "1. Importation des biblioth√®ques n√©cessaires    \n",
    "2. G√©n√©ration de la r√©ponse (mod√®le / politique) \n",
    "3. √âvaluation par retour humain (fonction de r√©compense)\n",
    "4. Boucle d‚Äôentra√Ænement (it√©rations/√©poques)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28654bc",
   "metadata": {},
   "source": [
    "##  1. Importations des biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d03003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c750b243",
   "metadata": {},
   "source": [
    "##  2.  G√©n√©ration de la r√©ponse (mod√®le / politique) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3373c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_text):\n",
    "    responses = [\"Bonjour !\", \"Salut !\", \"Je ne sais pas.\", \"Tr√®s bien.\"]\n",
    "    return random.choice(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de937930",
   "metadata": {},
   "source": [
    "##  3. üë§ √âvaluation par retour humain (fonction de r√©compense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback(response):\n",
    "    good_responses = [\"Bonjour !\", \"Tr√®s bien.\"]\n",
    "    return 1 if response in good_responses else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ed53e3",
   "metadata": {},
   "source": [
    "## 4. üîÑ Boucle d‚Äôentra√Ænement (it√©rations/√©poques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf98e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        input_text = \"Salut\"  \n",
    "        response = model(input_text)\n",
    "        reward = human_feedback(response)\n",
    "        print(f\"Epoch {epoch+1}: R√©ponse: {response} | R√©compense: {reward}\")\n",
    "        \n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86ca0ab",
   "metadata": {},
   "source": [
    "# 5. Pipeline Principal\n",
    "Voici l‚Äôex√©cution compl√®te du syst√®me RLHF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3407547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def model(input_text):\n",
    "    responses = [\"Bonjour !\", \"Salut !\", \"Je ne sais pas.\", \"Tr√®s bien.\"]\n",
    "    return random.choice(responses)\n",
    "def human_feedback(response):\n",
    "    good_responses = [\"Bonjour !\", \"Tr√®s bien.\"]\n",
    "    return 1 if response in good_responses else 0\n",
    "def train_model(epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        input_text = \"Salut\"  \n",
    "        response = model(input_text)\n",
    "        reward = human_feedback(response)\n",
    "        print(f\"Epoch {epoch+1}: R√©ponse: {response} | R√©compense: {reward}\")\n",
    "        \n",
    "\n",
    "train_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e78ee2",
   "metadata": {},
   "source": [
    "# 6. R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e29dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PS C:\\Users\\USER> & \"C:/Program Files/Python311/python.exe\" c:/Users/USER/moncode.py\n",
    "Epoch 1: R√©ponse: Je ne sais pas. | R√©compense: 0\n",
    "Epoch 2: R√©ponse: Salut ! | R√©compense: 0        \n",
    "Epoch 3: R√©ponse: Salut ! | R√©compense: 0        \n",
    "Epoch 4: R√©ponse: Salut ! | R√©compense: 0        \n",
    "Epoch 5: R√©ponse: Tr√®s bien. | R√©compense: 1     \n",
    "Epoch 6: R√©ponse: Bonjour ! | R√©compense: 1      \n",
    "Epoch 7: R√©ponse: Je ne sais pas. | R√©compense: 0\n",
    "Epoch 8: R√©ponse: Salut ! | R√©compense: 0        \n",
    "Epoch 9: R√©ponse: Tr√®s bien. | R√©compense: 1     \n",
    "Epoch 10: R√©ponse: Tr√®s bien. | R√©compense: 1    \n",
    "PS C:\\Users\\USER> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e262c30f",
   "metadata": {},
   "source": [
    "Ce code simule de mani√®re simple le principe du RLHF (Reinforcement Learning from Human Feedback) en g√©n√©rant al√©atoirement des r√©ponses √† une entr√©e fixe (\"Salut\") √† l‚Äôaide d‚Äôun faux mod√®le, puis en les √©valuant √† l‚Äôaide d‚Äôun retour humain simul√© (une fonction qui attribue une r√©compense de 1 si la r√©ponse est jug√©e \"bonne\", sinon 0). Lors de chaque it√©ration, le mod√®le g√©n√®re une r√©ponse, re√ßoit un feedback, et affiche la r√©compense obtenue. Bien que le mod√®le ne s‚Äôam√©liore pas r√©ellement au fil du temps (aucune mise √† jour n‚Äôest effectu√©e), ce code illustre le principe fondamental du RLHF : produire une sortie, recevoir un retour humain, et utiliser ce retour pour guider l‚Äôapprentissage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
