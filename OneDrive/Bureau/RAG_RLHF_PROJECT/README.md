# ğŸ“š Partie 1 : RAG - Retrieval-Augmented Generation

Ce dÃ©pÃ´t contient la premiÃ¨re partie d'un projet en deux volets, portant sur deux mÃ©thodes indÃ©pendantes d'optimisation de la gÃ©nÃ©ration de texte par modÃ¨les de langage.
Cette premiÃ¨re partie implÃ©mente un systÃ¨me **RAG (Retrieval-Augmented Generation)**, qui permet de rÃ©pondre Ã  des questions Ã  partir dâ€™un document PDF, en combinant recherche sÃ©mantique et gÃ©nÃ©ration de texte.

La seconde partie du projet portera sur la mÃ©thode **RLHF (Reinforcement Learning from Human Feedback)**, qui sera Ã©tudiÃ©e sÃ©parÃ©ment, avec d'autres modÃ¨les, d'autres donnÃ©es, et une approche distincte.


## ğŸŒŸ Objectif de cette partie (RAG)

Lâ€™objectif est de permettre Ã  un systÃ¨me de gÃ©nÃ©ration automatique de rÃ©pondre de maniÃ¨re fiable Ã  des questions, en sâ€™appuyant sur une base documentaire externe (ici : un fichier PDF), selon les Ã©tapes suivantes :

1. Extraction de texte depuis un PDF.
2. DÃ©coupage du texte en passages.
3. Embedding des passages avec un modÃ¨le `SentenceTransformer`.
4. Indexation vectorielle via FAISS.
5. Recherche sÃ©mantique des passages les plus pertinents.
6. GÃ©nÃ©ration de rÃ©ponse avec un modÃ¨le de type T5.



## ğŸ§° FonctionnalitÃ©s

* ğŸ“„ Lecture et traitement automatique de fichiers PDF.
* âœ‚ï¸ Segmentation du texte en paragraphes courts.
* ğŸ” Recherche sÃ©mantique Ã  lâ€™aide de vecteurs dâ€™embeddings.
* ğŸ§  GÃ©nÃ©ration de rÃ©ponses Ã  partir du contexte sÃ©lectionnÃ©.



## âš™ï¸ Utilisation

### 1. Installation des dÃ©pendances

```bash
pip install fitz numpy faiss-cpu sentence-transformers transformers
```

Remarque : `fitz` correspond Ã  `PyMuPDF`.



### 2. âš™ï¸ ExÃ©cution du script

Placez votre fichier PDF dans le rÃ©pertoire du projet, par exemple :
`definitions_generales.pdf`

Ensuite, exÃ©cutez le fichier Python principal avec la commande suivante :

```bash
python RAG_Project.py
```

Le script effectuera toutes les Ã©tapes nÃ©cessaires, de l'extraction du texte jusqu'Ã  la gÃ©nÃ©ration de la rÃ©ponse.


### 3. âœ… RÃ©sultat attendu

Le script effectuera les opÃ©rations suivantes :

* ğŸ“ **Extrait** le texte depuis le fichier PDF.
* âœ‚ï¸ **DÃ©coupe** le texte en paragraphes courts (\~200 caractÃ¨res max).
* ğŸ”¢ **CrÃ©e** des embeddings vectoriels Ã  partir de ces paragraphes.
* ğŸ” **Effectue** une recherche sÃ©mantique Ã  partir d'une question utilisateur.
* ğŸ§  **GÃ©nÃ¨re** une rÃ©ponse Ã  partir du contexte le plus pertinent.

**Lors de l'exÃ©cution, les sorties suivantes s'affichent dans la console :**

* âœ… **Texte extrait et dÃ©coupÃ©** : affichage des premiers paragraphes extraits.
* âœ… **Passages pertinents** : afichage des passages les plus proches de la question.
* âœ… **Contexte utilisÃ©** : affichage du contenu utilisÃ© pour gÃ©nÃ©rer la rÃ©ponse.
* âœ… **RÃ©ponse gÃ©nÃ©rÃ©e** : texte produit par le modÃ¨le.



### ğŸ§ª Exemple de question

Lors de l'exÃ©cution du script, vous pouvez poser une question comme :

```text
C'est quoi l'innovation ?
```


### ğŸ§± Composants utilisÃ©s

* **Embedding** : `all-MiniLM-L6-v2` (modÃ¨le prÃ©entrainÃ© de `sentence-transformers`)
* **Indexation** : `FAISS` (recherche vectorielle rapide)
* **GÃ©nÃ©ration** : `google/flan-t5-base` (via `transformers`, pipeline `"text2text-generation"`)

# ğŸ“š Partie 2 : RLHF - Reinforcement Learning from Human Feedback

Ce dÃ©pÃ´t contient la seconde partie du projet, axÃ©e sur la mÃ©thode RLHF (Reinforcement Learning from Human Feedback), une approche dâ€™apprentissage par renforcement guidÃ©e par des retours humains. Cette dÃ©monstration simplifiÃ©e illustre le fonctionnement conceptuel de RLHF sans modÃ¨les complexes, en se concentrant sur lâ€™interaction entre un agent, un retour humain, et lâ€™Ã©volution de ses comportements en fonction des rÃ©compenses.

## Objectif de cette partie (RLHF)

Lâ€™objectif est de simuler comment un agent peut apprendre Ã  produire de meilleures rÃ©ponses Ã  partir de feedbacks humains simulÃ©s, selon les Ã©tapes suivantes :

1.GÃ©nÃ©ration dâ€™une rÃ©ponse alÃ©atoire Ã  une entrÃ©e fixe.

2.Ã‰valuation de cette rÃ©ponse par une fonction de feedback humain.

3.RÃ©compense attribuÃ©e selon la qualitÃ© perÃ§ue de la rÃ©ponse.

4.Affichage de la rÃ©ponse et de sa rÃ©compense Ã  chaque itÃ©ration.

## ğŸ§° FonctionnalitÃ©s
ğŸ§  Simulation dâ€™un modÃ¨le de rÃ©ponse simple
ğŸ‘¤ Ã‰valuation manuelle simulÃ©e des rÃ©ponses
ğŸ” Boucle dâ€™entraÃ®nement avec retour de rÃ©compense
ğŸ“Š Affichage des performances par itÃ©ration

##  ExÃ©cution du script
 
            python RLHF_Project.py

## âœ… RÃ©sultat attendu    

Le script exÃ©cute 10 itÃ©rations dâ€™un entraÃ®nement simulÃ© :

ğŸ“¥ Le "modÃ¨le" reÃ§oit une entrÃ©e texte (ex. "Salut")

ğŸ¤– Il gÃ©nÃ¨re une rÃ©ponse alÃ©atoire parmi des phrases dÃ©finies

ğŸ§‘ Le systÃ¨me simule un retour humain pour chaque rÃ©ponse

ğŸ“ˆ Chaque rÃ©ponse est accompagnÃ©e de sa rÃ©compense (0 ou 1)

 Sortie exemple :

python-repl
Copier
Modifier
Epoch 1: RÃ©ponse: TrÃ¨s bien. | RÃ©compense: 1
Epoch 2: RÃ©ponse: Je ne sais pas. | RÃ©compense: 0

## Exemple dâ€™entrÃ©e
Aucune interaction requise : lâ€™entrÃ©e simulÃ©e est "Salut".

## ğŸ§± Composants utilisÃ©s
ModÃ¨le simulÃ© : fonction model() qui gÃ©nÃ¨re une rÃ©ponse alÃ©atoire

Feedback humain : fonction human_feedback() qui attribue une rÃ©compense

Boucle d'entraÃ®nement : train_model() rÃ©pÃ¨te lâ€™Ã©valuation 10 fois